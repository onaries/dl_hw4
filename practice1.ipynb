{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the read_record function from data_read file\n",
    "from data_read import read_record\n",
    "\n",
    "#the main imports\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set number of hidden nodes in each layer\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "#numner of classes. In our case it is positive, negative and neutral\n",
    "n_classes = 3\n",
    "\n",
    "batch_size=512\n",
    "#there are 1600000 lines in the training data\n",
    "total_batches = int(1600000/batch_size)\n",
    "#total number of epochs(epoch is number of times the whole data set is seen )\n",
    "hm_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholders for the input and output\n",
    "# x = tf.placeholder(tf.float32,shape=[None, 2638])\n",
    "x = tf.placeholder(tf.float32,shape=[None, 3007])\n",
    "y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the hidden layers' weights and biases\n",
    "#the 2638 is the number of words in the lexicon created using the training data\n",
    "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                #   'weight':tf.Variable(tf.random_normal([2638, n_nodes_hl1])),\n",
    "                  'weight':tf.Variable(tf.random_normal([3007, n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "#define the output layers weights and biases\n",
    "output_layer = {'f_fum':None,\n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The saver object\n",
    "#this can only be made after declaring some tensorflow variables.\n",
    "#in our case, those are the layer weights and biases above\n",
    "saver = tf.train.Saver()\n",
    "#log file to store number of epochs completed\n",
    "tf_log = 'tf.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    \"\"\"\n",
    "    Function to define the neural network model\n",
    "    In this version it is just a feedforward multi layer preceptrion\n",
    "    Args:\n",
    "        data: tensorflow placeholder containing the feature vector/s\n",
    "    Returns:\n",
    "        output: Output tensor built after the network\n",
    "    \"\"\"\n",
    "    #all of the layers follow the standard M * X + b function where M is the weights and b is the bias\n",
    "    #then the hidden layers are passed through the relu function\n",
    "    l1 = tf.nn.relu(tf.add(tf.matmul(data, hidden_1_layer['weight']), hidden_1_layer['bias']))\n",
    "    l2 = tf.nn.relu(tf.add(tf.matmul(l1, hidden_2_layer['weight']), hidden_2_layer['bias']))\n",
    "    l3 = tf.nn.relu(tf.add(tf.matmul(l2, hidden_3_layer['weight']), hidden_3_layer['bias']))\n",
    "    output = tf.add(tf.matmul(l3, output_layer['weight']),output_layer['bias'])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    \"\"\"\n",
    "    Function to train the neural network\n",
    "    Args:\n",
    "        x: the tensorflow placeholder for the input feature vector\n",
    "    \"\"\"\n",
    "    #the feed forward tensorflow operation\n",
    "    prediction = neural_network_model(x)\n",
    "    #cost operation\n",
    "    #tf.nn.softmax_cross_entropy_with_logits is used instead of tf.nn.softmax when we have one-hot vectors as both the labels and the output from the network\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,100000, 0.96, staircase=True)\n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    learning_step = (tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step))\n",
    "\n",
    "    #Define the optimizer and the value to minimize\n",
    "    #NOTE Following line is disabled to check decaying learning_rate\n",
    "    # optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    #queue of input files\n",
    "    filename_queue = tf.train.string_input_producer(['./data/shuffled_train_data.csv'],num_epochs=hm_epochs)\n",
    "    #get the lexicon built and saved previously from the training data\n",
    "    with open('./lexicon.pickle', 'rb') as f:\n",
    "        lexicon = pickle.load(f)\n",
    "    #tensorflow operations to get one tweet and one set of one-hot labels from the input file\n",
    "    tweet_op, label_op = read_record(filename_queue)\n",
    "\n",
    "    #All tensorflow operations need to be run from a session\n",
    "    with tf.Session() as sess:\n",
    "        #the tensorflow variables(eg; weights and biases declared earlier) have to be initialized using this function\n",
    "        #otherwise they are just tensors describing those variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #the epoch is counted internally in tensorflow. This counter needs to be initialized using this function\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        #try to get the epoch number from the log file\n",
    "        try:\n",
    "            epoch =int(open(tf_log,'r').read().split('\\n')[-2])+1\n",
    "            print(\"Starting: Epoch %d\" % epoch)\n",
    "        #if the file does not open, assume we are on epoch 1\n",
    "        except:\n",
    "            epoch = 1\n",
    "\n",
    "        try:\n",
    "            #if some epochs have already been run, then restore from checkpoint file called 'model.ckpt' using the saver\n",
    "            if epoch != 1:\n",
    "                saver.restore(sess, \"model.ckpt\")\n",
    "            epoch_loss=1\n",
    "            #the batch of input vectors\n",
    "            batch_x = []\n",
    "            #the batch of labels\n",
    "            batch_y = []\n",
    "            #to keep track of the number of batches run in one epoch\n",
    "            batches_run = 0\n",
    "            #the following two lines are essencial for using tensorflow's data reader from the file queue (filename_queue) above\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord = coord)\n",
    "            #the should_stop function keeps running the following code until the epochs in the filename_queue are completed\n",
    "            #the epoch counter is internal which is why we used the tf.initialize_local_variables earlier\n",
    "            while not coord.should_stop():\n",
    "                #run the operation to get one tweer as a binary string and one set of one-hot labels\n",
    "                raw_tweet, label = sess.run([tweet_op,label_op])\n",
    "                #convert the binary tweet to utf-8(python3's default string encoding)\n",
    "                tweet = str(raw_tweet,'utf-8')\n",
    "                #a numpy array of zeros representing, each representing one word in the lexicon\n",
    "                features = np.zeros(len(lexicon))\n",
    "                #if any word in the tweet is in the lexicon, we increment its count in the feature vector\n",
    "                for word in tweet:\n",
    "                    if word.lower() in lexicon:\n",
    "                        index_value = lexicon.index(word.lower())\n",
    "                        features[index_value] += 1\n",
    "                #add the tweet's feature vector to the batch\n",
    "                batch_x.append(list(features))\n",
    "                batch_y.append(label)\n",
    "                #when a batch is filled\n",
    "                if len(batch_x)>=batch_size:\n",
    "                    #run the optimizer operation and the cost operation using the batch\n",
    "                    _,c = sess.run([learning_step, cost], feed_dict={x:np.array(batch_x), y:np.array(batch_y)})\n",
    "                    #increment the epoch loss by the cost c of the batch\n",
    "                    epoch_loss += c\n",
    "                    #empty the batch\n",
    "                    batch_x = []\n",
    "                    batch_y = []\n",
    "                    #increment the batch counter\n",
    "                    batches_run += 1\n",
    "                    #print the label of the last example in the batch for debugging/sanity check\n",
    "                    # print('sample label: {}'.format(label))\n",
    "                    #show the batch loss and the number of batches run in the given epoch\n",
    "                    print('Batch run: {}/{} | Epoch: {} | Batch Loss: {}'.format(batches_run,total_batches, epoch, c))\n",
    "                if batches_run == total_batches: #meaning one epoch completed\n",
    "                    print(\"saving model\")\n",
    "                    saver.save(sess, 'model.ckpt') #save the variables at each epoch\n",
    "\n",
    "                    print('Epoch: {}, completed out of {}, Loss = {}'.format(epoch, hm_epochs, epoch_loss))\n",
    "                    with open(tf_log, 'a') as f:\n",
    "                        f.write(str(epoch)+'\\n')\n",
    "                        epoch +=1\n",
    "                    batches_run=0 #reset the counter\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Done training, epoch reached\")\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            threads.join(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network():\n",
    "    \"\"\"\n",
    "    Function to test the neural network\n",
    "    \"\"\"\n",
    "    #tensorflow op to get prediction\n",
    "    prediction = neural_network_model(x)\n",
    "    #tensorflow queue for the input file\n",
    "    filename_queue = tf.train.string_input_producer(['./data/test_data.csv'],num_epochs=1)\n",
    "    #tensorflow op to check if prediction is correct\n",
    "    #the argmax function gives the index of the maximum value on the given axis. In this case axis 1\n",
    "    #axis 1 meaning the row axis here\n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    #tensorflow operation to get the accuracy of the classifier\n",
    "    #it converts the result of the previous op from boolean to float32 and then takes the mean over the result\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    #lists for features and labels for the input\n",
    "    feature_sets = []\n",
    "    labels = []\n",
    "\n",
    "    #get the lexicon\n",
    "    with open('./lexicon.pickle','rb') as f:\n",
    "        lexicon = pickle.load(f)\n",
    "    #tensorflow operation to get one tweet and one set of labels\n",
    "    tweet_op, label_op = read_record(filename_queue)\n",
    "    with tf.Session() as sess:\n",
    "        #initialize the saved variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #this is required to initialize the internal epoch counter used by the filename_queue\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        #get the trained model or show an error\n",
    "        try:\n",
    "            # print(saver.latest_checkpoint())\n",
    "            saver.restore(sess,\"model.ckpt\")\n",
    "            print('restored network')\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "        try:\n",
    "            #tensorflow coordinator to coordinate the threads\n",
    "            coord = tf.train.Coordinator()\n",
    "            #tensorflow queue runner required to get input from the filename_queue\n",
    "            threads = tf.train.start_queue_runners(coord = coord)\n",
    "            #run the following until the number of epochs specified in filename_queue are completed\n",
    "            while not coord.should_stop():\n",
    "                #get one tweet as binary string and one set of labels\n",
    "                tweet_raw, label = sess.run([tweet_op,label_op])\n",
    "                #if you want to see the tweet, uncomment the line below\n",
    "                # print(tweet_raw)\n",
    "                #convert the tweet to utf-8, python3's default string encoding\n",
    "                tweet = str(tweet_raw, 'utf-8')\n",
    "                #uncomment below to see the converted tweet\n",
    "                # print(tweet)\n",
    "                #initialize a numpy array of zeros for each word in the lexicon\n",
    "                features = np.zeros(len(lexicon))\n",
    "                #if a word in the tweet is in the lexicon, increment the count of that word in the feature vector\n",
    "                for word in tweet:\n",
    "                    if word.lower() in lexicon:\n",
    "                        index_value = lexicon.index(word.lower())\n",
    "                        features[index_value] +=1\n",
    "                #append the feature vector and label to the input list\n",
    "                feature_sets.append(features)\n",
    "                labels.append(label)\n",
    "        #since the test file is small, the whole file is processed in the above lines\n",
    "        #when this is done we get an OutOfRangeError meaning the epochs specified have been completed\n",
    "        #in this case, we only need 1 epoch and when it is complete we classify the test data\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            #convert the lists to numpy arrays\n",
    "            test_x = np.array(feature_sets)\n",
    "            test_y = np.array(labels)\n",
    "            #for debugging pruposes, the shape of the numpy array can be checked\n",
    "            #the first dimension should equal the number of lines in the test csv\n",
    "            # print(test_x.shape)\n",
    "            # print(test_y.shape)\n",
    "            #uncomment below if you want to see the actual boolean array that shows which examples were correctly classified\n",
    "            # correct_example = sess.run(correct, feed_dict={x:feature_sets,y:labels})\n",
    "            # print(correct_example)\n",
    "            # print(correct_example.shape)\n",
    "            # get the accuracy by running the accuracy operation\n",
    "            # accuracy.eval is just a different way of saying sess.run(accuracy, feed_dict={x:test_x, y:test_y})\n",
    "            print('Accuracy:',accuracy.eval({x:test_x, y:test_y}))\n",
    "            print(\"Epoch complete\")\n",
    "        #stop the coordinator and join the threads\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            # threads.join(coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './lexicon.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8c170e2723a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#uncomment to train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# train_neural_network(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtest_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-37043edba1f4>\u001b[0m in \u001b[0;36mtest_neural_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#get the lexicon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./lexicon.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#tensorflow operation to get one tweet and one set of labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lexicon.pickle'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #uncomment to train the network\n",
    "    # train_neural_network(x)\n",
    "    test_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './lexicon.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1fe4c7422045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# import time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# start = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# fin = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# print(fin-start)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-12f207094505>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# init_process('./data/training.1600000.processed.noemoticon.csv','./data/train_data.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# init_process('./data/testdata.manual.2009.06.14.csv','./data/test_data.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./lexicon.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lexicon.pickle'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
